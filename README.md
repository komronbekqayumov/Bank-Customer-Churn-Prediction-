
---

## ğŸ§  Customer Churn Prediction

This project focuses on predicting **customer churn** using supervised machine learning algorithms. The dataset is preprocessed, enhanced with feature engineering, and evaluated across multiple models to identify the most accurate predictor.

---

### ğŸ“‚ Project Structure

```
â”œâ”€â”€ dataset.csv
â”œâ”€â”€ churn_prediction.ipynb
â”œâ”€â”€ final_predictions.csv
â”œâ”€â”€ models.pkl
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

### ğŸ›  Technologies Used

* Python
* Pandas, NumPy
* Scikit-learn
* XGBoost
* LightGBM
* Matplotlib, Seaborn
* Jupyter Notebook (Kaggle Environment)

---

### ğŸ”„ Workflow

1. **Data Preprocessing**

   * Handled missing values
   * Encoded categorical variables (LabelEncoding, OneHotEncoding)

2. **Feature Engineering**

   * Created new features such as `AgeBalanceRatio`, `IsSenior`, `HasHighBalance`, `ProductsPerYear`, and others to boost model performance

3. **Model Training & Evaluation**
   Models used:

   * Logistic Regression
   * Random Forest
   * XGBoost
   * LightGBM
   * K-Nearest Neighbors (KNN)
   * Support Vector Machine (SVM)

4. **Model Selection**

   * Accuracy scores were compared, and the best-performing model was identified

---

### âœ… Results

```text
Logistic Regression     â†’ Accuracy: 88.7%
Random Forest           â†’ Accuracy: 93.2%
XGBoost                 â†’ Accuracy: 92.4%
K-Nearest Neighbors     â†’ Accuracy: 91.1%
LightGBM (GBooster)     â†’ Accuracy: 92.3%
Support Vector Machine  â†’ Accuracy: 89.5%

ğŸ¯ Best Model: Random Forest Classifier  
âœ”ï¸ Accuracy: 93.2%
```

---

### ğŸ“ Output

Final predictions were saved to a CSV file:

```python
final_df.to_csv("final_predictions.csv", index=False)
```

---

### â–¶ï¸ How to Run

1. Clone the repository
2. Install dependencies

```bash
pip install -r requirements.txt
```

3. Open and run the notebook `churn_prediction.ipynb` in Jupyter or Kaggle

---

### ğŸ“¬ Contact

For questions or suggestions, feel free to open an issue or contact me via GitHub.

---

